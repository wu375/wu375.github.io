<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="UTF-8">
    <title>blog to self-introduce</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta name="description" content="quick review of what and how of SED with deep learning" />

    <meta property="og:title" content="sound event detection #1: what and how" />
    <meta property="og:description" content="quick review of what and how of SED with deep learning" />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="/posts/sed1/" />
    <meta itemprop="name" content="sound event detection #1: what and how">
    <meta itemprop="description" content="quick review of what and how of SED with deep learning">
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content="sound event detection #1: what and how"/>
    <meta name="twitter:description" content="quick review of what and how of SED with deep learning"/>

    <link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32.png">

    
    <link rel="stylesheet" href="/scss/style.min.436642b633ad0a0dc4a7a99ade60a2b76a56102682aeadc369780247dd463588.css" >
</head>
<body>
    <header>
    <div class="header poster-header-frame">
        
        
        
</header>



    <div id="content">
  <article class="post">
    
    
      <button class="floating-button">
    <a class="floating-button__link" href="">
        <span>back</span>
    </a>
</button>

      <div class="post-content"><h1 id="sound-event-detection-what-and-how">Sound event detection: what and how</h1>
<p>Layout:</p>
<ol>
<li>Problem definition</li>
<li>Deep learning Methods</li>
<li>Challenges</li>
<li>A real example</li>
<li>References</li>
</ol>
<h2 id="problem-definition">Problem definition</h2>
<p>Sound event detection (SED): Task to identify the type of sound event. May also <strong>identify start/end time</strong> in the long audio stream.</p>
<p>Like most problems, SED relies on labeling, or the start/end time locations. A concept called <strong>weak labeling</strong> exists in SED world, which refers to labels telling only types of sound events. A famous such dataset (named <strong>Audio Set</strong>) is by Google.</p>
<p>There is also a thing called 2017 DCASE challenge, which is a dataset and asks people to solve weakly labeled SED tasks.</p>
<p>How to predict? Easy:</p>
<ul>
<li>For each audio stream, predict binary label for each sound event at local substreams (or instances, frames&hellip;).</li>
<li>Combine results using a <strong>pooling function</strong></li>
<li>(This may be referred to as multiple instance learning)</li>
</ul>
<p>[1] talks about some pooling methods: Max, Average, Linear, Exp, Attention.</p>
<p>Guess what Max and Average pooling mean. The latter three are less obvious: at a high level some function to map y to [0,1].</p>
<p><em>A term—standard multiple instance (SMI)</em>: an assumption that if one segment (or instance) is positive, the entire audio stream is positive. Note that this is not guaranteed for options other than max pooling.</p>
<p>Question a: Wait. They use attention. The paper also anaylizes gradients for poolings. So pooling is part of training?</p>
<p>Answer a: Yes. This is (as I interpret) due to the fact that the problem is weakly supervised (so you only know the aggregated labels). In the <strong>supervised</strong> Kaggle example discussed later, however, the problem gives specific timeframes at which sound events occur/end, which may require slightly different approaches.</p>
<p>The paper says it used &ldquo;Filterbank features with input as 400 frames and 64 frequency bins&rdquo;</p>
<p>A quick recap of audio spectrograms powered by <a href="https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html">this awesome post</a> and <a href="https://www.youtube.com/watch?v=9GHCiiDLHQ4">this awesome Youtube video</a>:</p>
<ul>
<li>Why not just use waveform?
<ul>
<li>(Actually you could: e.g. wavenet)</li>
<li>Similarly perceived sounds can have very different waveform but similar spectrograms.</li>
</ul>
</li>
<li>Then why not just use STFT (time-frequency domain):
<ul>
<li>It assumes same level/scale of acoustic perception (in terms of pitch for example) at different frequencies</li>
<li>Human, however, perceives frequencies at log scale (clearly at low f).</li>
<li>This motivates Hz -&gt; Mel scale transformation</li>
</ul>
</li>
<li>Specifically, go with MFCCs or filterbanks in <strong>speech recognition</strong> to account for the above issue.</li>
<li>MFCCs breaks correlations between features (which may lose information), which is suitable for traditional ML algos.
<ul>
<li>Some say DCT used in the last can be replaced (or absorbed) by NNs because it&rsquo;s linear.</li>
</ul>
</li>
<li>This is not true for filterbanks. Used more in deep learning because NNs care less about independence.</li>
<li>To sum up computationally:
<ol>
<li>Have your time-series waveform</li>
<li>Do pre-emphasis (that adding x(t-1) thing)</li>
<li>STFT and hamming window</li>
<li>To mel scale and apply filterbanks (a matrix)</li>
<li>Log (saw someone says this is the key difference between filterbanks and mfcc, not DCF)</li>
<li>DCT (mfcc)</li>
<li>Normalize</li>
<li>Maybe something more</li>
</ol>
</li>
</ul>
<h2 id="some-more-cheap-talks-models-data-augs-before-the-real-deal">Some more cheap talks (models, data augs&hellip;) before the real deal</h2>
<p>Still, we need some plans on models, data augmentation, etc. to begin coding.</p>
<p>I took a look at the <a href="https://www.kaggle.com/c/rfcx-species-audio-detection">Kaggle example</a> coming next. Several people are using a pretrained model called PANNs [2]. So let&rsquo;s check it out.</p>
<p>PANNs:</p>
<ul>
<li>The paper tries several CNN architectures on an audio tagging task</li>
<li>Architectures include CNN, resnet, 1dCNN on waveform, mobileCNN</li>
<li>The 38 layer resnet was relatively good</li>
<li>A quite comprehensive paper with ablations/tuning on variables like hop size, data aug, sample rates and many more</li>
<li>Sample rate 32kHz. Window size 1024. Hop 320. 100 frames per second. 64 mel filterbanks. Cut-offs 50-14k Hz</li>
<li>Pytorch (sad)</li>
</ul>
<p>Good. Besides PANNs, annother person used resnet+imagenet weights.</p>
<p>Well, recently I&rsquo;m also trying to pratice use of transformers. So I also quickly went through these two [3] [4] <i>Oh my. It&rsquo;s already dark outside.</i></p>
<p>Transformers: As expected, both papers simply attach some transformer layers to the result of CNN.
[4] also asks transformer to output a scalar as pooling (though this is for weakly supervised case).</p>
<p>Naturally we think of attaching a transformer to PANNs or some pretrained resNets. But before that, we need resNets baseline working and we need to implement a transformer or find one on Github.</p>
<p>Finally, we need some data augmentations. Gaussian noise, timeshift, quadratic noise, even interpolation mix.</p>
<p>With all this, I&rsquo;ll start coding, details of which will be in the next part.</p>
<p>References:</p>
<p>[1] A COMPARISON OF FIVE MULTIPLE INSTANCE LEARNING POOLING FUNCTIONS FOR SOUND EVENT DETECTION WITH WEAK LABELING - Yun Wang, Juncheng Li, and Florian Metze<br>
[2] Kong, Q., Cao, Y., Iqbal, T., Wang, Y., Wang, W., &amp; Plumbley, M. D. (2020). PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition. ArXiv:1912.10211 [Cs, Eess]. <a href="http://arxiv.org/abs/1912.10211">http://arxiv.org/abs/1912.10211</a><br>
[3] Miyazaki, K., Komatsu, T., Hayashi, T., Watanabe, S., Toda, T., &amp; Takeda, K. (2020). Weakly-Supervised Sound Event Detection with Self-Attention. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 66–70. <a href="https://doi.org/10.1109/ICASSP40776.2020.9053609">https://doi.org/10.1109/ICASSP40776.2020.9053609</a><br>
[4]Kong, Q., Xu, Y., Wang, W., &amp; Plumbley, M. D. (2020). Sound Event Detection of Weakly Labelled Data with CNN-Transformer and Automatic Threshold Optimization. ArXiv:1912.04761 [Cs, Eess]. <a href="http://arxiv.org/abs/1912.04761">http://arxiv.org/abs/1912.04761</a></p>
</div>
    
    <button class="floating-button">
    <a class="floating-button__link" href="">
        <span>back</span>
    </a>
</button>

  </article>
  
    </div>
    
    
        
<script src="/js/script.js"></script>

      
  </body>
</html>
