<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ai on blog to self-introduce, rat race, get a skin-in-the-game in wording, etc.</title>
    <link>/tags/ai/</link>
    <description>Recent content in ai on blog to self-introduce, rat race, get a skin-in-the-game in wording, etc.</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 18 Jan 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>sound event detection: what and how</title>
      <link>/posts/sed1/</link>
      <pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/sed1/</guid>
      <description>Sound event detection: what and how Layout:
 Problem definition Deep learning Methods Challenges A real example References  Problem definition Sound event detection (SED): Task to identify the type of sound event. May also identify start/end time in the long audio stream.
Like most problems, SED relies on labeling, or the start/end time locations. A concept called weak labeling exists in SED world, which refers to labels telling only types of sound events.</description>
    </item>
    
    <item>
      <title>creative coding: how&#39;s it made?</title>
      <link>/posts/cc1/</link>
      <pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/cc1/</guid>
      <description>Creative coding: 1st attempt layout:
 creative coding and javascript generative learning  Creative coding and js Generative learning Generative arts, creative coding, generative coding.. These all fo some reason hint on the use of machine learning/deep learning. Actually they share one similarity. They are both, to some extent, abstract/mysterious/unexplainable. Arts are like neural networks, functions where inputs are images and outputs are, say, levels of satisfaction of viewers. It&amp;rsquo;s hard to tell what triggers different outputs, which is a digression to the ancient brain vs nn problem</description>
    </item>
    
    <item>
      <title>workshop: play text-based games with rl</title>
      <link>/posts/nlp_rl/</link>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/nlp_rl/</guid>
      <description>Play interactive fiction games with reinforcement learning *A report written for school, thus the (annoying) verboseness
*Chose the topic thanks to the Wordplay workshop at NeurIPS 2020 (did not attend, just reading papers)
Contents
 Natural language tool definition Data and environment specifications Problem and algorithms Experiments and comparisons References  1 Natural language tool definition
The natural language tool this report evaluates is a reinforcement learning (RL) agent that plays a type of game called Interactive Fiction (IF).</description>
    </item>
    
    <item>
      <title>VAE: reasoning process</title>
      <link>/posts/vae/</link>
      <pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/vae/</guid>
      <description>No math, just bullet points! to stopped being constantly bothered by theories behind VAE.
Start with GMM: each z corresponds to a cluster.
Optimizing log probability of GMM: wonâ€™t work because you have to sample the unique z that correspond to an x. Otherwise the other sampled p(x|z) are virtually zero (so no gradients).
Distributed representation: an important catch: e.g. binary features result in 2^n, rather than n, clusters.
Thus, the chance of sampling a useful z is 1/(2^n).</description>
    </item>
    
  </channel>
</rss>
