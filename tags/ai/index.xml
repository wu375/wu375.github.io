<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ai on blog to self-introduce</title>
    <link>/tags/ai/</link>
    <description>Recent content in ai on blog to self-introduce</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 29 Jan 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>sound event detection #2: kaggle is real</title>
      <link>/posts/sed2/</link>
      <pubDate>Fri, 29 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/sed2/</guid>
      <description>Sound event detection #2: Kaggle is REAL It&amp;rsquo;s funny seeing myself complaining about how long (an afternoon) it took to research SED before entering the competition. Because in the past 10 days, I spent 70% (to say the least) of all sitting time on the competition, reading, familiarizing, coding, training, inferring, failing. It&amp;rsquo;s addictive and people here are competitive and SERIOUS about the competition. And yet I still could not match the public baseline with 20 days to go (the public score now is 0.</description>
    </item>
    
    <item>
      <title>sound event detection #1: what and how</title>
      <link>/posts/sed1/</link>
      <pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/sed1/</guid>
      <description>Sound event detection: what and how Layout:
 Problem definition Deep learning Methods Challenges A real example References  Problem definition Sound event detection (SED): Task to identify the type of sound event. May also identify start/end time in the long audio stream.
Like most problems, SED relies on labeling, or the start/end time locations. A concept called weak labeling exists in SED world, which refers to labels telling only types of sound events.</description>
    </item>
    
    <item>
      <title>creative coding: how&#39;s it made?</title>
      <link>/posts/cc1/</link>
      <pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/cc1/</guid>
      <description>Creative coding: 1st attempt layout:
 creative coding and javascript generative learning  Creative coding and js The other day I came across this blog post by Professor Zach Lieberman, summarizing his journey creating one coding sketch everyday in 2017.
Generative learning Generative arts, creative coding, generative coding.. These all fo some reason hint on the use of machine learning/deep learning. Actually they share one similarity. They are both, to some extent, abstract/mysterious/unexplainable.</description>
    </item>
    
    <item>
      <title>workshop: play text-based games with rl</title>
      <link>/posts/nlp_rl/</link>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/nlp_rl/</guid>
      <description>Play interactive fiction games with reinforcement learning *A report written for school, thus the (annoying) verboseness
*Chose the topic thanks to the Wordplay workshop at NeurIPS 2020 (did not attend, just reading papers)
Contents
 Natural language tool definition Data and environment specifications Problem and algorithms Experiments and comparisons References  1 Natural language tool definition
The natural language tool this report evaluates is a reinforcement learning (RL) agent that plays a type of game called Interactive Fiction (IF).</description>
    </item>
    
    <item>
      <title>VAE: reasoning process</title>
      <link>/posts/vae/</link>
      <pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/vae/</guid>
      <description>No math, just bullet points! to stopped being constantly bothered by theories behind VAE.
Start with GMM: each z corresponds to a cluster.
Optimizing log probability of GMM: wonâ€™t work because you have to sample the unique z that correspond to an x. Otherwise the other sampled p(x|z) are virtually zero (so no gradients).
Distributed representation: an important catch: e.g. binary features result in 2^n, rather than n, clusters.
Thus, the chance of sampling a useful z is 1/(2^n).</description>
    </item>
    
  </channel>
</rss>
